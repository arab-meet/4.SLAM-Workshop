# SLAM Introduction

### What is SLAM?

**Simultaneous localization and mapping** ( **SLAM** ) is the problem of constructing a map of an unknown environment while simultaneously keeping track of the robot's pose within it.

To put it in simple terms imagine a robot moving in a room full of obstacles the robot doesn't know its initial pose (start point) as the robot moves it gathers data from the sensors mounted on it but these data are noisy here comes the SLAM problem as we need to construct a map and know where our robot is in this map depending on an unknown starting point and some noisy data from sensors.

SLAM is presented as a probabilistic problem as there is uncertainty in the path we take and the objects we see so our approaches of solving it often uses probability techniques.

SLAM is a diffucult problem as it doesn't only deal with static, and structured objects but it also addresses dynamic objects, and large scale environments.

SLAM is considered a chicken or egg problem as:

* We need a map for localization
* we need to know where the robot is in order to construct a map

The SLAM concept first appeared in Smith, Self and Cheeseman work in 1986 **[1]**. After this comes Hugh Durrant-Whyte and John J. Leonard in 1991 **[2]** to define the SLAM problem in their work which they called SMAL(Simultaneous mapping and localization) originally. the term SLAM itself first appeared in 1996 in ISR **[3]**.

---

### Localization vs Mapping vs SLAM

First of all we say that SLAM is simultaneously performing localizaion and mapping so we need to understand why do we need to solve these problems together and in order to do that we need to understand how does these two concepts depend on each other.

Let's assume this situation where we have a robot taking a turn in an environment full of obstacles and this robot has a wheel encoder that provides us with odom data while also having a LIDAR mounted on it providing laser scans for the environment around us.

First scenario we are going to assume is that the odom data and laser scans are a 100% accurate and i have zero uncertainty in my movement and what the robot sees.

in this case The result will look something like this where the robot knows exactly where it is at any time and also knows what are the obstacles around it with their exact position.

![base state](image/base_state.png)

> But in reality this is not the case as all sensors have uncertainities in their measurements also there maybe some environmental conditions that may affect the data that the robot receives.

#### Localization

So in the following situation we assume that the odom data have some errors in the measurements while the laser scan is 100% accurate. Also since we are only interested in localizing the robot we will assume that we have the map and the location of each obstacle in it.

in the first section of the image we see the actual path that the robot took in the second section we notice something interesting the data from odom is not accurate so the robot thinks he is in a different position (the blue robot) but since the robot knows the mab and the laser scan is 100% accurate. when the robot puts the measured distance from the obstacles on the map he is drawing (the blue dotted lines) he finds that these measurements doesn't relate to anything and because the robot also have the a map of the environment he relates these measurements to the nearest obstacle and correct its position.

![localization](image/Localization.png)

in this previous example we found that by having the map and a good laser scanner we can do the localization task fairly easy but this is not the case in real-life applications.

> but before we continue to the next section let's take a moment and understad how the correction step is done.

##### Correction step

Here the robot starts the movement at the reference frame (0,0) and as we see after the first time step the robot location is (5,-1) but our robot doesn't know this instead he thinks he is at (4.5,1) because of the odem error. the laser data is accurate as we said and we have a map of the environment so we know that the obstacle is at (7,3). so when the robot gets the laser measurements it finds that the robot's position relative to the obstacle frame is [ (5,-1) - (7,3) ] which is equal to (-2,-4) but in reality the estimated position is [ (4.5,1) - (7,3) ] which equals (-2.5,-2) relative to the obstacle frame. but since we are a 100% sure that the laser data is accurate so (-2,-4) must be the correct coordinates so the robot start correcting itself and change its estimate to the right coordinates.

> **so in real world scenarios we counter these kind of errors a lot where i have different data from laser and odom but in real-world scenarios I'm not a 100% sure of anything around the robot so here comes the SLAM to provide you with a solution.**

![Correnction step](image/Correnction_step.png)

#### Mapping

Opposite to the previous situation here we assume that the robot has a 100% accuracy in the odom data but there are some uncertainity in the laser measurements and we also doesn't have the map . actually that's our task to build a map of the environment. as we can see from the two sections in the following image the path of the robot didn't change but since its laser data isn't accurate it located the obstacles in the wrong locations, hence creating a wrong and inaccurate map. maybe you think that these errors are not a big deal it's just a small misplacement but in compact and feature-packed environments this leads to a completely different map that we can't rely on.

![mapping](image/mapping.png)

#### SLAM

And finally the SLAM problem here we assume that the robot doesn't have any prior information about the map and its location and also the sensors of the robot are not 100% accurate but the odom and laser measurements have uncertainities in them. So what happens then let's take the same senario that we did before in the first time step before the robot starts moving it takes a laser scan of the environment but since there is some error in the laser data the robot is not sure where the obstacle is so it indicates the probabilty of its position with this beige oval around the obstacle saying that the obstacle cold be anywhere in this oval. After the robot starts moving in the second time step we notice that the position of the robot also has a probability because the odom data has errors in it and also the probability distribtion for the second obstacle increases significantly because of the uncertainity of the robot location so the errors accumilate over time. finally in the third time step we notice that the uncertainity in the robot location increases but that of the obstacle stays the same and this is because we already seen this obstacle in the previous time step so we already know where it could be.

![SLAM](image/SLAM.png)

> **As you have seen this is the SLAM problem and this is what we are going to learn how to solve in this documentation.**

---

### Definintion of SLAM problem

First let's define what is the output we need from SLAM. We mainly need to do two things:

* Localization: to know where the robot is and let's call this x(t) this term indicates the pose of the robot at any time t during the motion. the pose of the robot in 2D for example represents the posintion in x,y axes and orientation around z axis.
* Mapping: to describe the environment around the robot and what does the robot sees and interacts with. let's refere to the map as m. this variable holds information such as what are the objects surrounding our robot and what are the positions of these objects.

After we determined what exactly do we need to obtain from the SLAM process we need to know what are the information that we have and will help us with our problem and we also have to main information that we can use:

* Sensors' measurements: the information we get from sensors mounted on our robot such as Lidar, GPS, and Imu. we primarly rely on Laser data which describes the information between the objects surrounding the robot (m) and the position of the robot (x(t)). let's refer to the measurements in our formula as Z(t).
* Control: the control is the description of the motion of the robot between two consecutive poses and we get this information from the odometry data. we can obtain this information either from sensors mounted on the robot such as wheel encoder or by the controls we give to the robot directly if we can ensure that the robot will excute the movements commands precisely. let's call this term u(t).

After we defined the given parameters and the desired output that we want we can write the formula for the SLAM problem as follows:
![SLAM](image/SLAM_formula.png)

> this equation reads as follows: **the probability distribution of the robot poses at discrete point of time and map given the observation and controls**

we notice something in this equation which is that the the observations and controls start at time stamp 1:T where as the path of the robot starts at 0:T and this is because we need to assign a starting point which serves as the reference frame to the path of the robot. this reference frame is x(0) and it usually have zeros in the position and orientation attributes.

We can also represenet the SLAM problem as a graphical model.
![Graphical representation](image/Graphical_representation.png)

here we notice that the variable in white circles are what we want to obtian and the peige circles are the given parameters. the arrows represent the relation ship between the different parameters in the graph.

We also notice that in order to determine the current pose of our robot x(t) we need to know the previous pose x(t-1) and the control that we did u(t). the observations that the robot sees z(t) debends on the current pose of the robot x(t) and the map that we navigate in m.

### Why is SLAM a hard problem

SLAM is considered a hard problem because of two main reasons which are:

##### Error Accumulation

all sensors that we use in real life have errors even if these errors are very small that we can ignore them these errors accumulate over time and the gab between the sensors' measurements and the ground truth data only widens.
![error accumulation](image/error_accumulation.png)

As we see here the in time step 1 the robot is sure of its position as this is the reference frame. the robot takes an observation and it's not sure of the exact position of the obstacle so it gives it a probability distribution represnted by the ellipse behind the obstacle.

In time step 2 we notice that the robot also isn't sure of its position so it gives it a probability distribution. the robot also observes the same obstacle but since the robot already estimated where this obstacle was so it doesn't change the raduis of the ellipse.

However in time step 3 we can see that our uncertainty in the robot's pose only grows larger. also when the robot observed another obstacle we add the uncertainity of the observation data to the error in the pose of the robot.

we can see here how we started in the first observation with a small uncertainty for the first obstacle and after only two time steps the uncertainty became more than double the value.

we can minimize the error accumulation by using filters such as EKF also by using multiple sensors and performing sensor fusion.

An important concept in SLAM that considers this problem and is designed to solve it is loop closure which we will explore further in this document.

##### Wrong data association

The second critical problem in SLAM is associating data to wrong objects as this is considerd a fatal mistake and is extremely hard to contain after it happens. to understand this problem let's look at this picture.
![data association](image/data_association.png)

In section 1 of the picture we see the path that our robot took and the obstacle in the map that it should observes.

In section 2 we notice that in time step 1 and 2 the robot observes the obstacle successfully and assign a probability distribution to it but something different happens at time step 3 we see that the robot have an error while observing the same object and didn't identify it correctly so the robot decided that this is another object and add it to the mab. this resulted in creating a map that is completely different from the actual map and this happened with only one object on the map so imagine the robot navigating in an environment full of obstacles.

this problem is minimized by adding some constraints when defining an object and relating this observation to previous observations.

As we said the wrong association of objects is a hard problem because for our example the robot created a whole new obstacle in the map and we want it to discover that this obstacle is a mistake and it should be deleted. so it's really important to prevent this from happening in the first place.

we will also mention this problem in the loop closure section.

#### Loop closure

Loop closure as we said is a key ascpect in SLAM and as the name suggests it means that whenever we find a loop after gathering some data we close this loop. To fully understand the loop closure approach we will explain it in a graph-based SLAM manner which we will encounter ahead in this course but we will just give a quick explanation now.

So let's assume our robot is moving in a hospital and while it's moving around it's building a map of this hospital and it's recording it's poses along the route it took and suddenly the robot came back to the starting position or passed by a point that it has visited before but it noticed something wrong the robot is getting the same laser data from before so he knows he has been here before but the estimated position in the map is not the same as before. this happens because the robot may have slipped along the way or the odometry data is not that accurate but these inaccurate data will result in the wrong map so when the robot reach a point that it has seen before he closes the loop and corrects the wrong route thereby correcting the map.

![Loop_closure](image/Loop_closure.gif)

you may not fully understand the concept of loop closure now but it's ok. as we said we will get in details regarding this subject in the graph-based SLAM session.

You can watch this video [here](https://youtu.be/saVZtgPyyJQ?si=XSB4b50qpWb3dS48) that explain the loop closure concept simply.

---

### Full SLAM vs online SLAM

#### Full SLAM

Full SLAM computes the complete set of poses over discrete time-steps from start until the current timestamp, along with the map of the environment. It is very computationally expensive (both in memory use and time) as it re-computes the complete sequence of robot poses.

In Online SLAM, the goal is to estimate only the current pose x(t) and the map m, without keeping track of the entire trajectory. This is done by solving the recursive problem:

![Full_SLAM](image/Full_SLAM.png)

#### Online SLAM

Online SLAM is a subset of the above and only computes the current robot pose. All computations are done recursively and several algorithms  are used to update previous measurements due to loop-closure or other significant events.

In Online SLAM, the goal is to estimate only the current pose xtx_t**x**t and the map mm**m**, without keeping track of the entire trajectory. This is done by solving the recursive problem:

![Online_SLAM](image/Online_SLAM.png)

Here's a table comparing **Full SLAM** and  **Online SLAM** :

| **Aspect**               | **Full SLAM**                               | **Online SLAM**                              |
| ------------------------------ | ------------------------------------------------- | -------------------------------------------------- |
| **Main focus**           | Entire robot trajectory and map optimization      | Current robot pose and map update in real-time     |
| **Data used**            | All past data from sensors                        | Only recent data                                   |
| **Computational demand** | High, due to large-scale optimization             | Lower, designed for real-time performance          |
| **Accuracy**             | High accuracy, as all data is considered          | Lower accuracy, prioritizing real-time speed       |
| **Use case**             | Offline processing, post-mission mapping          | Real-time applications, autonomous navigation      |
| **Best suited for**      | Scenarios where precision is crucial              | Dynamic environments requiring immediate decisions |
| **Memory usage**         | Large, as it stores and optimizes full trajectory | Limited, only processes recent data                |

> Both Full SLAM and Online SLAM are modeled using  **Bayesian filtering** , but the scope and formulation differ.

### Different approaches of SLAM

##### Filter Based SLAM

Filter based SLAM treats the SLAM problem as a state-estimation problem, where
 the state encompasses information about the current position and map.
The state is recursively updated by a filter, which estimates the
current position and map based on actions and measurements. As more data
 is collected, the estimate is augmented and refined.

##### Graph Based SLAM

Graph
 based SLAM treats the SLAM problem as a graph problem, where position
information is represented by the nodes and the map is derived with the
edges. In the robotics space, it’s common to use pose-graph
optimization. In a pose graph, the nodes represent poses and landmarks
and the edges represent constraints between them. New nodes are added as
 new poses and landmarks are detected, and constraints connect
sequential nodes with information about the movement. For example,
moving from one point to another would be represented with two nodes,
each containing information about the pose and landmarks measured,
connected by an edge, containing information about the motion and other
observations. In constructing the pose graph, we can also add edges
between nodes if they are similar enough, indicating a return to some
previous poses and landmarks. When two nodes are very similar, it
provides information to update the pose graph and potentially add new
edges indicating their closeness

##### Deep Learning Based SLAM

Deep
 learning based SLAM attempts to solve the SLAM problem using neural
networks and deep learning. As of writing this article, deep learning
based SLAM is nascent, however there’s a lot of promise given the
success of deep learning for other tasks and computer vision objectives
(e.g., [neural networks achieve state of the art performance in object classification](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)).
 Deep learning can be used throughout the SLAM problem, from describing
features to aligning different measurements to making estimates.
