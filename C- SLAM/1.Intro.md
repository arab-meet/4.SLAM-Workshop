# SLAM Introduction

### What is SLAM?

**Simultaneous localization and mapping** ( **SLAM** ) is the problem of constructing a map of an unknown environment while simultaneously keeping track of the robot's pose within it.

To put it in simple terms imagine a robot moving in a room full of obstacles the robot doesn't know its initial pose (start point) as the robot moves it gathers data from the sensors mounted on it but these data are noisy here comes the SLAM problem as we need to construct a map and know where our robot is in this map depending on an unknown starting point and some noisy data from sensors.

SLAM is presented as a probabilistic problem as there is uncertainty in the path we take and the objects we see so our approaches of solving it often uses probability techniques.

SLAM is a diffucult problem as it doesn't only deal with static, and structured objects but it also addresses dynamic objects, and large scale environments.

SLAM is considered a chicken or egg problem as:

* We need a map for localization
* we need to know where the robot is in order to construct a map

The SLAM concept first appeared in Smith, Self and Cheeseman work in 1986 **[1]**. After this comes Hugh Durrant-Whyte and John J. Leonard in 1991 **[2]** to define the SLAM problem in their work which they called SMAL(Simultaneous mapping and localization) originally. the term SLAM itself first appeared in 1996 in ISR **[3]**.

### Localization vs Mapping vs SLAM

First of all we say that SLAM is simultaneously performing localizaion and mapping so we need to understand why do we need to solve these problems together and in order to do that we need to understand how does these two concepts depend on each other.

Let's assume this situation where we have a robot taking a turn in an environment full of obstacles and this robot has a wheel encoder that provides us with odom data while also having a LIDAR mounted on it providing laser scans for the environment around us.

First scenario we are going to assume is that the odom data and laser scans are a 100% accurate and i have zero uncertainty in my movement and what the robot sees.

in this case The result will look something like this where the robot knows exactly where it is at any time and also knows what are the obstacles around it with their exact position.

![base state](image/base_state.png)

But in reality this is not the case as all sensors have uncertainities in their measurements also there maybe some environmental conditions that may affect the data that the robot receives.

#### Localization

So in the following situation we assume that the odom data have some errors in the measurements while the laser scan is 100% accurate. Also since we are only interested in localizing the robot we will assume that we have the map and the location of each obstacle in it.

in the first section of the image we see the actual path that the robot took in the second section we notice something interesting the data from odom is not accurate so the robot thinks he is in a different position (the blue robot) but since the robot knows the mab and the laser scan is 100% accurate. when the robot puts the measured distance from the obstacles on the map he is drawing (the blue dotted lines) he finds that these measurements doesn't relate to anything and because the robot also have the a map of the environment he relates these measurements to the nearest obstacle and correct its position.

![localization](image/Localization.png)

in this previous example we found that by having the map and a good laser scanner we can do the localization task fairly easy but this is not the case in real-life applications.

but before we continue to the next section let's take a moment and understad how the correction step is done.

##### Correction step

Here the robot starts the movement at the reference frame (0,0) and as we see after the first time step the robot location is (5,-1) but our robot doesn't know this instead he thinks he is at (4.5,1) the laser data is accurate as we said and we have a map of the environment so we know that the obstacle is at (7,3). so when the robot gets the laser measurements it finds that it 

![Correnction step](image/Correnction_step.png)

![mapping](image/mapping.png)
![SLAM](image/SLAM.png)

### Definintion of SLAM problem

First let's define what is the output we need from SLAM. We mainly need to do two things:

* Localization: to know where the robot is and let's call this x(t) this term indicates the pose of the robot at any time t during the motion. the pose of the robot in 2D for example represents the posintion in x,y axes and orientation around z axis.
* Mapping: to describe the environment around the robot and what does the robot sees and interacts with. let's refere to the map as m. this variable holds information such as what are the objects surrounding our robot and what are the positions of these objects.

After we determined what exactly do we need to obtain from the SLAM process we need to know what are the information that we have and will help us with our problem and we also have to main information that we can use:

* Sensors' measurements: the information we get from sensors mounted on our robot such as Lidar, GPS, and Imu. we primarly rely on Laser data which describes the information between the objects surrounding the robot (m) and the position of the robot (x(t)). let's refer to the measurements in our formula as Z(t).
* Control: the control is the description of the motion of the robot between two consecutive poses and we get this information from the odometry data. we can obtain this information either from sensors mounted on the robot such as wheel encoder or by the controls we give to the robot directly if we can ensure that the robot will excute the movements commands precisely. let's call this term u(t).

After we defined the given parameters and the desired output that we want we can write the formula for the SLAM problem as follows:
![SLAM](image/SLAM_formula.png)

> this equation reads as follows: **the probability distribution of the robot poses at discrete point of time and map given the observation and controls**

we notice something in this equation which is that the the observations and controls start at time stamp 1:T where as the path of the robot starts at 0:T and this is because we need to assign a starting point which serves as the reference frame to the path of the robot. this reference frame is x(0) and it usually have zeros in the position and orientation attributes.

We can also represenet the SLAM problem as a graphical model.
![Graphical representation](image/Graphical_representation.png)

here we notice that the variable in white circles are what we want to obtian and the peige circles are the given parameters. the arrows represent the relation ship between the different parameters in the graph.

We also notice that in order to determine the current pose of our robot x(t) we need to know the previous pose x(t-1) and the control that we did u(t). the observations that the robot sees z(t) debends on the current pose of the robot x(t) and the map that we navigate in m.

### Why is SLAM a hard problem

SLAM is considered a hard problem because of two main reasons which are:

##### Error Accumulation

all sensors that we use in real life have errors even if these errors are very small that we can ignore them these errors accumulate over time and the gab between the sensors' measurements and the ground truth data only widens.
![error accumulation](image/error_accumulation.png)

As we see here the in time step 1 the robot is sure of its position as this is the reference frame. the robot takes an observation and it's not sure of the exact position of the obstacle so it gives it a probability distribution represnted by the ellipse behind the obstacle.

In time step 2 we notice that the robot also isn't sure of its position so it gives it a probability distribution. the robot also observes the same obstacle but since the robot already estimated where this obstacle was so it doesn't change the raduis of the ellipse.

However in time step 3 we can see that our uncertainty in the robot's pose only grows larger. also when the robot observed another obstacle we add the uncertainity of the observation data to the error in the pose of the robot.

we can see here how we started in the first observation with a small uncertainty for the first obstacle and after only two time steps the uncertainty became more than double the value.

we can minimize the error accumulation by using filters such as EKF also by using multiple sensors and performing sensor fusion.

An important concept in SLAM that considers this problem and is designed to solve it is loop closure which we will explore further in this document.

##### Wrong data association

The second critical problem in SLAM is associating data to wrong objects as this is considerd a fatal mistake and is extremely hard to contain after it happens. to understand this problem let's look at this picture.
![data association](image/data_association.png)

In section 1 of the picture we see the path that our robot took and the obstacle in the map that it should observes.

In section 2 we notice that in time step 1 and 2 the robot observes the obstacle successfully and assign a probability distribution to it but something different happens at time step 3 we see that the robot have an error while observing the same object and didn't identify it correctly so the robot decided that this is another object and add it to the mab. this resulted in creating a map that is completely different from the actual map and this happened with only one object on the map so imagine the robot navigating in an environment full of obstacles.

this problem is minimized by adding some constraints when defining an object and relating this observation to previous observations.

As we said the wrong association of objects is a hard problem because for our example the robot created a whole new obstacle in the map and we want it to discover that this obstacle is a mistake and it should be deleted. so it's really important to prevent this from happening in the first place.

we will also mention this problem in the loop closure section.

#### Loop closure

### Full SLAM vs online SLAM

Full SLAM computes the complete set of poses over discrete time-steps
 from start until the current timestamp, along with the map of the
environment. It is very computationally expensive (both in memory use
and time) as it re-computes the complete sequence of robot poses i.e.,
the path.

Online SLAM is a subset of the above and only computes the current
robot pose. All computations are done recursively and several algorithms
 are used to update previous measurements due to loop-closure or other
significant events.

![1725905387508](image/README/1725905387508.png)

### Different approaches of SLAM

##### Filter Based SLAM

Filter based SLAM treats the SLAM problem as a state-estimation problem, where
 the state encompasses information about the current position and map.
The state is recursively updated by a filter, which estimates the
current position and map based on actions and measurements. As more data
 is collected, the estimate is augmented and refined.

##### Graph Based SLAM

Graph
 based SLAM treats the SLAM problem as a graph problem, where position
information is represented by the nodes and the map is derived with the
edges. In the robotics space, it’s common to use pose-graph
optimization. In a pose graph, the nodes represent poses and landmarks
and the edges represent constraints between them. New nodes are added as
 new poses and landmarks are detected, and constraints connect
sequential nodes with information about the movement. For example,
moving from one point to another would be represented with two nodes,
each containing information about the pose and landmarks measured,
connected by an edge, containing information about the motion and other
observations. In constructing the pose graph, we can also add edges
between nodes if they are similar enough, indicating a return to some
previous poses and landmarks. When two nodes are very similar, it
provides information to update the pose graph and potentially add new
edges indicating their closeness

##### Deep Learning Based SLAM

Deep
 learning based SLAM attempts to solve the SLAM problem using neural
networks and deep learning. As of writing this article, deep learning
based SLAM is nascent, however there’s a lot of promise given the
success of deep learning for other tasks and computer vision objectives
(e.g., [neural networks achieve state of the art performance in object classification](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)).
 Deep learning can be used throughout the SLAM problem, from describing
features to aligning different measurements to making estimates.
